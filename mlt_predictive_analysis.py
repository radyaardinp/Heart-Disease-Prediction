# -*- coding: utf-8 -*-
"""MLT_Predictive analysis

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ptGasaxHraVyt-VU0TL-Ig1UYHOukPxo

# **Klasifikasi Penyakit Jantung**
- **Nama:** Radya Ardi Ninang Pudyastuti
- **Dataset:** https://www.kaggle.com/datasets/rashadrmammadov/heart-disease-prediction

# Import Library

Langkah pertama adalah melakukan import library yang dibutuhkan
"""

#Import library python
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split, learning_curve
from sklearn.preprocessing import MinMaxScaler
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report
from google.colab import drive

"""# Loading Data

Langkah selanjutnya adalah melakukan mounting Google Drive dan membaca dataset CSV ke dalam DataFrame df untuk analisis.
"""

#membaca dataset
drive.mount('/content/drive')
dataset_path = '/content/drive/MyDrive/TUGAS CODING CAMP/ML Terapan/dataset_heart_disease.csv'

# Read the dataset into a pandas DataFrame
df = pd.read_csv(dataset_path)
df

"""kemudian memeriksa ukuran data untuk mengetahui berapa jumlah baris dan kolom pada dataset yang dimiliki"""

# Memeriksa ukuran data
df.shape

"""# Data Preprocessing

Memeriksa nilai kosong yang ada pada dataset
"""

#Memeriksa missing value
df.isnull().sum()

"""dari hasil di atas, terdapat missing value pada kolom 'Alcohol Intake'. nilai kosong tersebut harus dilakukan penanganan

selanjutnya adalah memeriksa tipe data. apakah tipe data yang ada sudah sesuai atau belum. jika belum sesuai, maka akan dilakukan penyesuaian tipe data
"""

#Memeriksa tipe data
df.info()

"""selanjutnya adala memeriksa nilai duplikat"""

#Memeriksa nilai duplikat
df.duplicated().sum()

"""dari hasil di atas, pada dataset heart disease prediction tidak memiliki nilai yang duplikat

Membuat visualisasi dan menampilkan jumlah masing-masing kelas untuk melihat keseimbangan data antara penderita dan non-penderita penyakit jantung.
"""

# Checking the balance of classes (Heart Disease vs. No Heart Disease)
plt.figure(figsize=(6, 4))
sns.countplot(x='Heart Disease', data=df)
plt.title('Class Distribution of Heart Disease')
plt.xlabel('Heart Disease')
plt.ylabel('Count')
plt.xticks([0, 1], ['No Heart Disease', 'Heart Disease'])
plt.show()

# Display the counts numerically
class_counts = df['Heart Disease'].value_counts()
print("Class distribution:\n", class_counts)

"""Membuat visualisasi untuk melihat distribusi umur pasien keseluruhan"""

# visualisasi distribusi umur
plt.figure(figsize=(10, 6))
sns.histplot(data=df, x='Age', kde=True, bins=30)
plt.title('Distribution of Age')
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.show()

"""selanjutnya adalah membuat grafik batang untuk melihat distribusi fitur kategorikal terhadap kondisi penyakit jantung."""

import matplotlib.pyplot as plt
import seaborn as sns

cat_features = ['Gender', 'Smoking', 'Alcohol Intake', 'Family History',
                'Diabetes', 'Obesity', 'Exercise Induced Angina', 'Chest Pain Type']

for col in cat_features:
    plt.figure(figsize=(6, 4))
    ax = sns.countplot(data=df, x=col, hue='Heart Disease')

    # Tambahkan jumlah di atas tiap bar
    for p in ax.patches:
        height = p.get_height()
        ax.annotate(f'{height}',
                    (p.get_x() + p.get_width() / 2., height),
                    ha='center', va='bottom', fontsize=9, color='black', xytext=(0, 2),
                    textcoords='offset points')

    # Tambahkan judul dan format
    plt.title(f'{col} vs Heart Disease')
    plt.xticks(rotation=45)

    # Ubah legenda agar jelas (0 = Tidak Sakit, 1 = Sakit)
    handles, labels = ax.get_legend_handles_labels()
    labels = ['No Heart Disease', 'Has Heart Disease']
    ax.legend(handles, labels, title='Heart Disease')

    plt.tight_layout()
    plt.show()

#Encoding data/mengubah data menjadi numerikal
categorical_cols = df.select_dtypes(include=['object']).columns.tolist()
numerical_cols = df.select_dtypes(include=['int', 'float']).columns.tolist()

print("Categorical column:", categorical_cols)
print("Numerical column:", numerical_cols)

"""Selanjutnya adalah Mengubah data kategorikal menjadi format numerik menggunakan LabelEncoder agar bisa digunakan oleh algoritma machine learning.


"""

# Columns to encode
cols_to_encode = ['Gender', 'Smoking', 'Alcohol Intake', 'Family History', 'Diabetes', 'Obesity', 'Exercise Induced Angina', 'Chest Pain Type']

# Initialize LabelEncoder
label_encoders = {}
for col in cols_to_encode:
    label_encoders[col] = LabelEncoder()
    df[col] = label_encoders[col].fit_transform(df[col])

print("DataFrame after encoding:")
df.head()

"""Langkah selanjutnya adalah melakukan penanganan missing value pada kolom alcohol intake dengan nilai mean."""

# Mengatasi missing value pada kolom 'Alcohol Intake' dengan nilai mean
mean_alcohol_intake = df['Alcohol Intake'].mean()
df['Alcohol Intake'].fillna(mean_alcohol_intake, inplace=True)

# Memeriksa kembali missing value setelah pengisian
print("\nMissing values after imputation:")
print(df.isnull().sum())

"""Membuat boxplot untuk mengidentifikasi outlier pada fitur numerik seperti usia, tekanan darah, kolesterol, dan fitur numerik lainnya."""

#Memeriksa outlier pada fitur selain kategorikal, yaitu fitur 'Age', 'Cholesterol', 'Blood Pressure', 'Heart Rate', 'Exercise Hours', 'Stress Level', 'Blood Sugar'

plt.figure(figsize=(12, 8))
sns.boxplot(data=df[['Age', 'Cholesterol', 'Blood Pressure', 'Heart Rate', 'Exercise Hours', 'Stress Level', 'Blood Sugar']])

# Add title and labels
plt.title('Boxplots of Variables')
plt.xlabel('Variables')
plt.ylabel('Values')

# Show the plot
plt.show()

"""Sebelum masuk ke dalam tahap pemodelan.data akan dipisahkan fitur (X) dan target (y), lalu melakukan normalisasi fitur menggunakan MinMaxScaler agar berada dalam rentang 0â€“1.

"""

# Pisahkan fitur dan target
X = df.drop('Heart Disease', axis=1)
y = df['Heart Disease']

#Melakukan scalling dengan menggunakan metode MinMax Scaler
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)
X_scaled

df_scaled = pd.DataFrame(X_scaled, columns=X.columns)
df_scaled.head()

"""DataFrame di atas merupakan hasil dari transformasi minmax scaling pada setiap atribut/fitur pada dataset heart disease.

langkah terakhir sebelum pemodelan adalah Membagi data menjadi data latih dan data uji dengan proporsi 80:20 untuk keperluan pelatihan dan evaluasi model.
"""

# Bagi data menjadi train dan test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train.shape, X_test.shape

y_train.shape, y_test.shape

"""# Pemodelan

Pada tahap ini, akan dilakukan pemodelan menggunakan tiga algoritma klasifikasi, yaitu Support Vector Machine (SVM), K-Nearest Neighbors (KNN), dan Logistic Regression (LR). Setiap model akan dilatih dan dievaluasi untuk melihat performanya dalam mendeteksi penyakit jantung.
Model dengan akurasi terbaik selanjutnya akan digunakan untuk proses hyperparameter tuning guna meningkatkan performa prediksi secara optimal.

#### Support Vector Machine (SVM)
"""

# Inisialisasi model SVM
svm_model = SVC(random_state=42)

# Melatih model SVM
svm_model.fit(X_train, y_train)

# Memprediksi pada data test
y_pred_svm = svm_model.predict(X_test)

# Evaluasi model SVM
print("SVM Model Evaluation:")
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_svm))
print("\nAccuracy:", accuracy_score(y_test, y_pred_svm))
print("\nClassification Report:\n", classification_report(y_test, y_pred_svm))

#Membuat learning curve
train_sizes, train_scores, test_scores = learning_curve(
    svm_model, X, y, cv=5, n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 10))

train_scores_mean = np.mean(train_scores, axis=1)
train_scores_std = np.std(train_scores, axis=1)
test_scores_mean = np.mean(test_scores, axis=1)
test_scores_std = np.std(test_scores, axis=1)

plt.figure(figsize=(10, 6))
plt.title("Learning Curve (SVM)")
plt.xlabel("Training Examples")
plt.ylabel("Score")
plt.grid()

plt.fill_between(train_sizes, train_scores_mean - train_scores_std,
                 train_scores_mean + train_scores_std, alpha=0.1,
                 color="r")
plt.fill_between(train_sizes, test_scores_mean - test_scores_std,
                 test_scores_mean + test_scores_std, alpha=0.1, color="g")
plt.plot(train_sizes, train_scores_mean, 'o-', color="r",
         label="Training score")
plt.plot(train_sizes, test_scores_mean, 'o-', color="g",
         label="Cross-validation score")

plt.legend(loc="best")
plt.show()

"""Hasil learning curve di atas menunjukkan kinerja model SVM yang terus meningkat seiring bertambahnya jumlah data latih. Skor pada data pelatihan dan validasi cenderung mendekati dan stabil di angka tinggi, menandakan bahwa model memiliki kemampuan generalisasi yang baik tanpa indikasi overfitting atau underfitting yang signifikan.

#### Logistic Regression (LR)
"""

# Inisialisasi model Logistic Regression
lr_model = LogisticRegression(random_state=42, max_iter=1000) # Increase max_iter if convergence warnings occur

# Melatih model Logistic Regression
lr_model.fit(X_train, y_train)

# Memprediksi pada data test
y_pred_lr = lr_model.predict(X_test)

# Evaluasi model Logistic Regression
print("Logistic Regression Model Evaluation:")
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_lr))
print("\nAccuracy:", accuracy_score(y_test, y_pred_lr))
print("\nClassification Report:\n", classification_report(y_test, y_pred_lr))

#earning curve data for Logistic Regression
train_sizes_lr, train_scores_lr, test_scores_lr = learning_curve(
    lr_model, X, y, cv=5, n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 10))

train_scores_mean_lr = np.mean(train_scores_lr, axis=1)
train_scores_std_lr = np.std(train_scores_lr, axis=1)
test_scores_mean_lr = np.mean(test_scores_lr, axis=1)
test_scores_std_lr = np.std(test_scores_lr, axis=1)

# Plot learning curve for Logistic Regression
plt.figure(figsize=(10, 6))
plt.title("Learning Curve (Logistic Regression)")
plt.xlabel("Training Examples")
plt.ylabel("Score")
plt.grid()

plt.fill_between(train_sizes_lr, train_scores_mean_lr - train_scores_std_lr,
                 train_scores_mean_lr + train_scores_std_lr, alpha=0.1,
                 color="r")
plt.fill_between(train_sizes_lr, test_scores_mean_lr - test_scores_std_lr,
                 test_scores_mean_lr + test_scores_std_lr, alpha=0.1, color="g")
plt.plot(train_sizes_lr, train_scores_mean_lr, 'o-', color="r",
         label="Training score")
plt.plot(train_sizes_lr, test_scores_mean_lr, 'o-', color="g",
         label="Cross-validation score")

plt.legend(loc="best")
plt.show()

"""Learning curve untuk model Logistic Regression ini menunjukkan bahwa performa model cukup stabil baik pada data pelatihan maupun validasi. Terdapat sedikit gap antara skor pelatihan dan validasi, namun tidak terlalu besar, yang menandakan bahwa model mampu melakukan generalisasi dengan baik meskipun tidak sebaik model SVM.

#### K-Nearest Neighbour (KNN)
"""

# Inisialisasi model KNN
# Anda bisa mencoba beberapa nilai k (jumlah tetangga terdekat)
knn_model = KNeighborsClassifier(n_neighbors=5) # Contoh menggunakan k=5

# Melatih model KNN
knn_model.fit(X_train, y_train)

# Memprediksi pada data test
y_pred_knn = knn_model.predict(X_test)

# Evaluasi model KNN
print("KNN Model Evaluation:")
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_knn))
print("\nAccuracy:", accuracy_score(y_test, y_pred_knn))
print("\nClassification Report:\n", classification_report(y_test, y_pred_knn))

# learning curves for KNN
train_sizes_knn, train_scores_knn, test_scores_knn = learning_curve(
    knn_model, X_train, y_train, cv=5, n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 10))

# Calculate mean and standard deviation of training and testing scores for KNN
train_scores_mean_knn = np.mean(train_scores_knn, axis=1)
train_scores_std_knn = np.std(train_scores_knn, axis=1)
test_scores_mean_knn = np.mean(test_scores_knn, axis=1)
test_scores_std_knn = np.std(test_scores_knn, axis=1)

# Plot learning curves for KNN
plt.figure(figsize=(10, 6))
plt.fill_between(train_sizes_knn, train_scores_mean_knn - train_scores_std_knn,
                 train_scores_mean_knn + train_scores_std_knn, alpha=0.1, color="r")
plt.fill_between(train_sizes_knn, test_scores_mean_knn - test_scores_std_knn,
                 test_scores_mean_knn + test_scores_std_knn, alpha=0.1, color="g")
plt.plot(train_sizes_knn, train_scores_mean_knn, 'o-', color="r",
         label="Training score")
plt.plot(train_sizes_knn, test_scores_mean_knn, 'o-', color="g",
         label="Cross-validation score")

plt.title("Learning Curve (KNN)")
plt.xlabel("Training examples")
plt.ylabel("Score")
plt.legend(loc="best")
plt.grid()
plt.show()

"""Learning curve untuk model K-Nearest Neighbors (KNN) menunjukkan bahwa model memiliki skor pelatihan yang tinggi dan terus meningkat seiring bertambahnya data, namun terdapat kesenjangan yang cukup signifikan dengan skor validasi. Hal ini mengindikasikan bahwa model cenderung mengalami overfitting, yaitu sangat baik dalam mempelajari data pelatihan tetapi kurang optimal dalam melakukan generalisasi terhadap data baru.

-----------
selanjutnya akan dilakukan perbandingan akurasi dari ketiga model
"""

#Membandingkan hasil akurasi dari 3 pemodelan
results = {
    'Model': ['Logistic Regression', 'SVM', 'KNN'],
    'Accuracy': [accuracy_score(y_test, y_pred_lr), accuracy_score(y_test, y_pred_svm), accuracy_score(y_test, y_pred_knn)]
}

results_df = pd.DataFrame(results)

print("Perbandingan Akurasi Model LR, SVM, dan KNN:")
results_df

"""Dari tabel di atas didapatkan model SVM memiliki nilai akurasi yang lebih tinggi daripada model Logistic Regression dan KNN. Setelah itu akan dilakukan optimalisasi model menggunakan hyperparameter tuning GridSearch untuk mendapatkan nilai parameter dan akurasi yang paling optimal dari model SVM

# Optimalisasi Model

Setelah SVM terbukti sebagai model terbaik, dilakukan hyperparameter tuning menggunakan GridSearchCV untuk mengoptimalkan kinerjanya. Parameter yang dituning meliputi nilai C (regularisasi), gamma (koefisien kernel), dan jenis kernel yang digunakan.
"""

#Optimalisasi dengan cara hyperparameter tuning pada model terbaik menggunakan GridSearchCV
from sklearn.model_selection import GridSearchCV

# Define the parameter grid for SVM
param_grid = {
    'C': [0.1, 1, 10, 100],         # Regularization parameter
    'gamma': [1, 0.1, 0.01, 0.001], # Kernel coefficient
    'kernel': ['rbf', 'linear']     # Specifies the kernel type
}

# Initialize GridSearchCV
grid_search = GridSearchCV(SVC(), param_grid, refit=True, verbose=2, cv=5)

# Fit the grid search to the data
grid_search.fit(X_train, y_train)

# Print the best parameters and best score
print("Best Parameters found by GridSearchCV:", grid_search.best_params_)
print("Best Cross-validation Accuracy:", grid_search.best_score_)

# Get the best model
best_svm_model = grid_search.best_estimator_

# Evaluate the best model on the test set
y_pred_best_svm = best_svm_model.predict(X_test)

print("\nOptimized SVM Model Evaluation:")
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_best_svm))
print("\nAccuracy:", accuracy_score(y_test, y_pred_best_svm))
print("\nClassification Report:\n", classification_report(y_test, y_pred_best_svm))

# Compare the accuracy of the optimized SVM with previous models
results = {
    'Model': ['Logistic Regression', 'SVM (Initial)', 'KNN', 'SVM (Optimized)'],
    'Accuracy': [accuracy_score(y_test, y_pred_lr), accuracy_score(y_test, y_pred_svm), accuracy_score(y_test, y_pred_knn), accuracy_score(y_test, y_pred_best_svm)]
}

results_df = pd.DataFrame(results)

print("\nPerbandingan Akurasi Model Setelah Optimalisasi SVM:")
results_df

# Generate learning curves for the optimized SVM model
train_sizes_opt_svm, train_scores_opt_svm, test_scores_opt_svm = learning_curve(
    best_svm_model, X_train, y_train, cv=5, n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 10))

# Calculate mean and standard deviation of training and testing scores for optimized SVM
train_scores_mean_opt_svm = np.mean(train_scores_opt_svm, axis=1)
train_scores_std_opt_svm = np.std(train_scores_opt_svm, axis=1)
test_scores_mean_opt_svm = np.mean(test_scores_opt_svm, axis=1)
test_scores_std_opt_svm = np.std(test_scores_opt_svm, axis=1)

# Plot learning curves for optimized SVM
plt.figure(figsize=(10, 6))
plt.fill_between(train_sizes_opt_svm, train_scores_mean_opt_svm - train_scores_std_opt_svm,
                 train_scores_mean_opt_svm + train_scores_std_opt_svm, alpha=0.1, color="r")
plt.fill_between(train_sizes_opt_svm, test_scores_mean_opt_svm - test_scores_std_opt_svm,
                 test_scores_mean_opt_svm + test_scores_std_opt_svm, alpha=0.1, color="g")
plt.plot(train_sizes_opt_svm, train_scores_mean_opt_svm, 'o-', color="r",
         label="Training score (Optimized SVM)")
plt.plot(train_sizes_opt_svm, test_scores_mean_opt_svm, 'o-', color="g",
         label="Cross-validation score (Optimized SVM)")

plt.title("Learning Curve (Optimized SVM)")
plt.xlabel("Training examples")
plt.ylabel("Score")
plt.legend(loc="best")
plt.grid()
plt.show()

"""Learning curve dari model SVM yang telah dioptimasi menunjukkan performa pelatihan yang konsisten sangat tinggi mendekati 1.0, serta peningkatan stabil pada skor validasi hingga mendekati 0.93. Hal ini mengindikasikan bahwa model telah belajar dengan baik tanpa overfitting yang berlebihan, serta peningkatan akurasi validasi sebagai hasil dari proses tuning parameter."""

# menyimpan model SVM yang telah dilakukan tuning

import joblib

# Save the best performing model (optimized SVM)
model_filename = '/content/drive/MyDrive/TUGAS CODING CAMP/ML Terapan/optimized_svm_model.pkl'
joblib.dump(best_svm_model, model_filename)

print(f"Optimized SVM model saved to {model_filename}")